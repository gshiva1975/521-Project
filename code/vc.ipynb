{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Video Classification with LoRA Fine-Tuning and LangGraph-Based Video Summarization\n",
    "Author: GangadharSShiva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: INSTALLATION AND SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langgraph typing_extensions torch torchvision peft\n",
    "!pip install -q moviepy SpeechRecognition pydub\n",
    "!apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2: IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet18\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import matplotlib.pyplot as plt\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "import speech_recognition as sr\n",
    "from pydub import AudioSegment\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from IPython.display import Video, display\n",
    "from google.colab import drive\n",
    "\n",
    "# LangGraph Imports\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 3: GOOGLE DRIVE SETUP AND DATA EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define paths\n",
    "zip_file_path_in_drive = '/content/drive/My Drive/aai-521/videos-ds.zip'\n",
    "destination_folder = '/content/drive/My Drive/aai-521/videos-ds'\n",
    "train_data_dir = os.path.join(destination_folder, 'train/')\n",
    "test_data_dir = os.path.join(destination_folder, 'test/')\n",
    "train_csv_path = os.path.join(destination_folder, 'train.csv')\n",
    "test_csv_path = os.path.join(destination_folder, 'test.csv')\n",
    "\n",
    "# Create destination folder if needed\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Copy and unzip data\n",
    "if os.path.exists(zip_file_path_in_drive):\n",
    "    !cp \"{zip_file_path_in_drive}\" \"{destination_folder}/videos-ds.zip\"\n",
    "    print(\"✓ videos-ds.zip loaded from Google Drive\")\n",
    "    \n",
    "    zip_file_path = os.path.join(destination_folder, \"videos-ds.zip\")\n",
    "    !unzip -o -q \"{zip_file_path}\" -d \"{destination_folder}\"\n",
    "    print(\"✓ Data extracted successfully\")\n",
    "    \n",
    "    # Verify files\n",
    "    avi_files = glob.glob(os.path.join(destination_folder, '**', '*.avi'), recursive=True)\n",
    "    print(f\"✓ Found {len(avi_files)} video files\")\n",
    "else:\n",
    "    print(\"✗ Error: videos-ds.zip not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 4: UTILITY FUNCTIONS (Video Frame and DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, num_frames=16):\n",
    "    \"\"\"Extract evenly-spaced frames from video.\"\"\"\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return frames\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if total_frames == 0:\n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "    \n",
    "    for i in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function to filter None values and ensure consistent frame count.\"\"\"\n",
    "    batch = [item for item in batch \n",
    "             if item is not None and item[0] is not None and item[1] is not None]\n",
    "    \n",
    "    if not batch:\n",
    "        return None, None\n",
    "    \n",
    "    inputs, labels_or_paths = zip(*batch)\n",
    "    \n",
    "    # Check frame count consistency\n",
    "    expected_frames = 16\n",
    "    if any(inp.size(0) != expected_frames for inp in inputs):\n",
    "        print(f\"⚠ Skipping batch: inconsistent frame count (expected {expected_frames})\")\n",
    "        return None, None\n",
    "    \n",
    "    return torch.stack(inputs), list(labels_or_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 5: DATASET CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    \"\"\"Custom dataset for video classification.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, csv_path, num_frames=16, transform=None, is_test=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.videos = []\n",
    "        self.labels = []\n",
    "        self.is_test = is_test\n",
    "        self.video_paths = []\n",
    "        \n",
    "        df = pd.read_csv(csv_path)\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            video_name = row['video_name']\n",
    "            if not self.is_test:\n",
    "                self.labels.append(row['tag'])\n",
    "            \n",
    "            video_path = os.path.join(data_dir, video_name)\n",
    "            if os.path.exists(video_path):\n",
    "                self.videos.append(video_path)\n",
    "                if self.is_test:\n",
    "                    self.video_paths.append(video_path)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.videos[idx]\n",
    "        frames = extract_frames(video_path)\n",
    "        \n",
    "        if not frames:\n",
    "            return None, (video_path if self.is_test else None)\n",
    "        \n",
    "        frames = torch.from_numpy(np.array(frames)).permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        if self.transform:\n",
    "            frames = torch.stack([self.transform(frame) for frame in frames])\n",
    "        \n",
    "        if not self.is_test:\n",
    "            return frames, self.labels[idx]\n",
    "        else:\n",
    "            return frames, video_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 6: DATA LOADING (Video Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((112, 112)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = VideoDataset(train_data_dir, train_csv_path, transform=transform, is_test=False)\n",
    "test_dataset = VideoDataset(test_data_dir, test_csv_path, transform=transform, is_test=True)\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"✓ Training samples: {len(train_dataset)}\")\n",
    "print(f\"✓ Test samples: {len(test_dataset)}\")\n",
    "print(f\"✓ Number of classes: {len(set(train_dataset.labels))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 7: MODEL SETUP WITH LORA (Video Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"✓ Using device: {device}\")\n",
    "\n",
    "# Load pre-trained ResNet-18\n",
    "num_classes = len(set(train_dataset.labels))\n",
    "model = resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Apply LoRA\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"conv1\", \"layer1.0.conv1\", \"layer1.0.conv2\", \"layer1.1.conv1\", \"layer1.1.conv2\",\n",
    "        \"layer2.0.conv1\", \"layer2.0.conv2\", \"layer2.0.downsample.0\", \"layer2.1.conv1\", \"layer2.1.conv2\",\n",
    "        \"layer3.0.conv1\", \"layer3.0.conv2\", \"layer3.0.downsample.0\", \"layer3.1.conv1\", \"layer3.1.conv2\",\n",
    "        \"layer4.0.conv1\", \"layer4.0.conv2\", \"layer4.0.downsample.0\", \"layer4.1.conv1\", \"layer4.1.conv2\",\n",
    "        \"fc\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"✓ Model configured with LoRA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 8: TRAINING (OPTIONAL - UNCOMMENT TO TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num_epochs = 10\n",
    "# model.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(train_dataloader):\n",
    "#         if data is None or data[0] is None:\n",
    "#             continue\n",
    "#         \n",
    "#         inputs, labels = data\n",
    "#         unique_labels = sorted(list(set(train_dataset.labels)))\n",
    "#         label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "#         labels = torch.tensor([label_to_int[l] for l in labels], dtype=torch.long).to(device)\n",
    "#         inputs = inputs.to(device)\n",
    "#         \n",
    "#         optimizer.zero_grad()\n",
    "#         # Treat all frames in the batch as individual images for the 2D CNN (ResNet)\n",
    "#         outputs = model(inputs.view(-1, 3, 112, 112))\n",
    "#         # Average the frame-level predictions to get a video-level prediction\n",
    "#         outputs = outputs.view(inputs.size(0), inputs.size(1), -1).mean(dim=1)\n",
    "#         \n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         \n",
    "#         running_loss += loss.item()\n",
    "#         if (i + 1) % 10 == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}], Loss: {running_loss/10:.4f}')\n",
    "#             running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 9: EVALUATION (Video Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VIDEO CLASSIFICATION EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "video_paths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        if data is None or data[0] is None:\n",
    "            continue\n",
    "        \n",
    "        inputs, paths = data\n",
    "        # Use torch.stack to ensure consistent tensor structure if collate_fn returned a list of tensors\n",
    "        inputs = torch.stack(inputs).to(device)\n",
    "        \n",
    "        outputs = model(inputs.view(-1, 3, 112, 112))\n",
    "        outputs = outputs.view(inputs.size(0), inputs.size(1), -1).mean(dim=1)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "        video_paths.extend(paths)\n",
    "\n",
    "# Map predictions to labels\n",
    "unique_labels = sorted(list(set(train_dataset.labels)))\n",
    "label_map = {i: label for i, label in enumerate(unique_labels)}\n",
    "predicted_labels = [label_map[pred] for pred in predictions]\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'video_path': video_paths, \n",
    "    'predicted_label': predicted_labels\n",
    "})\n",
    "\n",
    "print(\"\\n✓ Predictions on test videos:\")\n",
    "print(results_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 10: LANGGRAPH-BASED VIDEO SUMMARIZATION\n",
    "This section uses LangGraph to orchestrate the multi-step summarization process: Audio Extraction -> Transcription -> Summary Video Creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the State of the LangGraph Agent\n",
    "class SummaryState(TypedDict):\n",
    "    video_path: str\n",
    "    audio_path: str\n",
    "    summary_video_path: str\n",
    "    transcript: str\n",
    "    duration: float\n",
    "    summary_clips_count: int\n",
    "    error: str\n",
    "\n",
    "# --- 10.1 LangGraph Nodes (Functions) ---\n",
    "\n",
    "def extract_audio(state: SummaryState) -> SummaryState:\n",
    "    \"\"\"Node 1: Extracts audio from the video and updates the state.\"\"\"\n",
    "    print(\"\\n▶ 1. Executing Audio Extraction...\")\n",
    "    video_path = state['video_path']\n",
    "    audio_path = video_path.replace('.avi', '.wav')\n",
    "    \n",
    "    try:\n",
    "        video_clip = VideoFileClip(video_path)\n",
    "        video_clip.audio.write_audiofile(audio_path, logger=None)\n",
    "        video_clip.close()\n",
    "        print(\"✓ Audio extracted.\")\n",
    "        return {**state, \"audio_path\": audio_path, \"error\": None}\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Audio extraction failed: {e}\")\n",
    "        return {**state, \"error\": f\"Audio extraction failed: {e}\"}\n",
    "\n",
    "\n",
    "def transcribe_audio(state: SummaryState) -> SummaryState:\n",
    "    \"\"\"Node 2: Transcribes the extracted audio and updates the state with the transcript.\"\"\"\n",
    "    print(\"▶ 2. Executing Transcription...\")\n",
    "    audio_path = state.get('audio_path')\n",
    "    \n",
    "    if not audio_path or state.get('error'):\n",
    "        return {**state, \"transcript\": \"Skipped due to prior error.\", \"error\": state.get('error')}\n",
    "        \n",
    "    transcript = \"No speech detected in video\"\n",
    "    try:\n",
    "        r = sr.Recognizer()\n",
    "        with sr.AudioFile(audio_path) as source:\n",
    "            audio_data = r.record(source)\n",
    "            transcript = r.recognize_google(audio_data)\n",
    "        print(f\"✓ Transcript generated: {transcript[:50]}...\")\n",
    "        return {**state, \"transcript\": transcript, \"error\": None}\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Transcription failed: {e} (assuming no speech)\")\n",
    "        return {**state, \"transcript\": transcript, \"error\": None}\n",
    "\n",
    "\n",
    "def create_summary_video(state: SummaryState) -> SummaryState:\n",
    "    \"\"\"Node 3: Extracts key frames and creates the summary video.\"\"\"\n",
    "    print(\"▶ 3. Executing Summary Video Creation...\")\n",
    "    video_path = state['video_path']\n",
    "    summary_video_path = video_path.replace('.avi', '_summary.mp4')\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    cap.release()\n",
    "    \n",
    "    if fps == 0:\n",
    "        return {**state, \"error\": \"FPS is zero, cannot calculate duration/frames.\"}\n",
    "        \n",
    "    duration = frame_count / fps\n",
    "    frame_indices = list(range(0, frame_count, 15)) # Extract frame every 15 frames\n",
    "    segment_duration = 1 # 1 second clip per key frame\n",
    "    summary_clips = []\n",
    "    \n",
    "    try:\n",
    "        for frame_idx in frame_indices:\n",
    "            start_time = frame_idx / fps\n",
    "            end_time = min(start_time + segment_duration, duration)\n",
    "            \n",
    "            if end_time > start_time:\n",
    "                clip = VideoFileClip(video_path).subclip(start_time, end_time)\n",
    "                summary_clips.append(clip)\n",
    "\n",
    "        if summary_clips:\n",
    "            final_summary = concatenate_videoclips(summary_clips)\n",
    "            final_summary.write_videofile(summary_video_path, codec='libx264', \n",
    "                                           audio_codec='aac', logger=None)\n",
    "            final_summary.close()\n",
    "            print(f\"✓ Summary video saved: {summary_video_path}\")\n",
    "            \n",
    "            return {**state, \n",
    "                    \"summary_video_path\": summary_video_path, \n",
    "                    \"duration\": duration, \n",
    "                    \"summary_clips_count\": len(summary_clips), \n",
    "                    \"error\": None\n",
    "                   }\n",
    "        else:\n",
    "            return {**state, \"error\": \"No clips extracted for summary.\"}\n",
    "    except Exception as e:\n",
    "        return {**state, \"error\": f\"Summary creation failed: {e}\"}\n",
    "\n",
    "# --- 10.2 Build and Run the LangGraph ---\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(SummaryState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"extract_audio\", extract_audio)\n",
    "workflow.add_node(\"transcribe_audio\", transcribe_audio)\n",
    "workflow.add_node(\"create_summary_video\", create_summary_video)\n",
    "\n",
    "# Set edges\n",
    "workflow.add_edge('extract_audio', 'transcribe_audio')\n",
    "workflow.add_edge('transcribe_audio', 'create_summary_video')\n",
    "workflow.add_edge('create_summary_video', END)\n",
    "\n",
    "# Set the starting point\n",
    "workflow.set_entry_point('extract_audio')\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# Run the graph on a sample video\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LANGGRAPH EXECUTION START\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample_video_path = '/content/drive/My Drive/aai-521/videos-ds/test/v_CricketShot_g01_c01.avi'\n",
    "initial_state = {\n",
    "    'video_path': sample_video_path,\n",
    "    'audio_path': '',\n",
    "    'summary_video_path': '',\n",
    "    'transcript': '',\n",
    "    'duration': 0.0,\n",
    "    'summary_clips_count': 0,\n",
    "    'error': None\n",
    "}\n",
    "\n",
    "final_state = app.invoke(initial_state)\n",
    "\n",
    "# --- 10.3 Display Final Results ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LANGGRAPH FINAL SUMMARY OUTPUT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if final_state.get('error'):\n",
    "    print(f\"❌ ERROR: {final_state['error']}\")\n",
    "else:\n",
    "    print(f\"Video Path: {final_state['video_path']}\")\n",
    "    print(f\"Transcript: {final_state['transcript']}\")\n",
    "    print(f\"Original duration: {final_state['duration']:.2f}s\")\n",
    "    print(f\"Summary duration: {final_state['summary_clips_count']}s\")\n",
    "    \n",
    "    try:\n",
    "        display(Video(final_state['summary_video_path'], embed=True, width=640))\n",
    "    except:\n",
    "        print(f\"Video saved but cannot display inline: {final_state['summary_video_path']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
